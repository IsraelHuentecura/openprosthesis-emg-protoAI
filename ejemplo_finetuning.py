### 2. Script de Ejemplo para Fine-tuning
import os
import json
import numpy as np
import tensorflow as tf
from pathlib import Path

# ------------------------------------------------------------------------- #
# üß† EJEMPLO DE FINE-TUNING PARA HyT-Net
# ------------------------------------------------------------------------- #
# El fine-tuning consiste en tomar un modelo pre-entrenado, congelar la
# mayor√≠a de sus capas (el "cuerpo" que extrae caracter√≠sticas) y re-entrenar
# √∫nicamente las √∫ltimas capas (la "cabeza" clasificadora) con una tasa de
# aprendizaje muy baja. Es √∫til para adaptar un modelo a nuevos datos o
# para intentar exprimir un poco m√°s de rendimiento.
# ------------------------------------------------------------------------- #

# --- 1. Configuraci√≥n ---
SAVE_ROOT = Path("saved_datasets")
MODELS_DIR = Path("models_h5")
FINETUNE_MODEL_NAME = "HyT-Net_fold1.h5"  # Modelo que vamos a cargar y mejorar
FINETUNE_EPOCHS = 10                     # Entrenar por pocas √©pocas
FINETUNE_LR = 1e-5                       # Tasa de aprendizaje muy baja

# --- Capa de Atenci√≥n (necesaria si el modelo la usa) ---
# Nota: Tu modelo HyT-Net usa MultiHeadAttention, que es una capa est√°ndar
# de Keras. Si tienes otro modelo que usa una capa 'Attention' personalizada,
# deber√°s mantener esta clase. Para HyT-Net no es estrictamente necesaria
# al momento de cargar el modelo si no se guard√≥ con ella.
class Attention(tf.keras.layers.Layer):
    def build(self, inp_shape):
        u = inp_shape[-1]
        self.W = tf.keras.layers.Dense(u, use_bias=True, name="att_W")
        self.u = tf.keras.layers.Dense(1, use_bias=False, name="att_u")
    def call(self, x):
        v = tf.tanh(self.W(x))
        vu = tf.squeeze(self.u(v), -1)
        al = tf.nn.softmax(vu, axis=1)
        return tf.reduce_sum(x * tf.expand_dims(al, -1), 1)

# --- 2. Carga de Datos ---
print("üíæ Cargando datos de entrenamiento y prueba...")
# Aseg√∫rate de que las rutas a tus datasets son correctas
if not (SAVE_ROOT / "train_ds_h").exists() or not (SAVE_ROOT / "test_ds_h").exists():
     raise FileNotFoundError(f"Los directorios del dataset no se encontraron en '{SAVE_ROOT}'.")
train_ds_h = tf.data.Dataset.load(str(SAVE_ROOT / "train_ds_h"))
test_ds_h = tf.data.Dataset.load(str(SAVE_ROOT / "test_ds_h"))
print("‚úÖ Datos cargados.")

# --- 3. Cargar Modelo Pre-entrenado ---
model_path = MODELS_DIR / FINETUNE_MODEL_NAME
if not model_path.exists():
    raise FileNotFoundError(f"El modelo '{model_path}' no existe. Aseg√∫rate de tener los modelos .h5.")

print(f"\nüß† Cargando modelo base: {model_path}")
# Se carga sin compilar para poder modificarlo.
# Si tu modelo no usa la capa 'Attention' personalizada, puedes remover `custom_objects`.
base_model = tf.keras.models.load_model(model_path, custom_objects={'Attention': Attention}, compile=False)
base_model.summary() # Imprime un resumen para verificar los nombres de las capas

# --- 4. Congelar Capas Base ---
print("\n‚ùÑÔ∏è  Congelando capas del modelo base...")
# Congelamos TODAS las capas primero
for layer in base_model.layers:
    layer.trainable = False

# Descongelamos las √öLTIMAS capas para re-entrenarlas (el clasificador de HyT-Net)
# **CORRECCI√ìN:** Se usan los nombres de las capas del clasificador final de tu modelo HyT-Net.
# Keras puede a√±adir un sufijo num√©rico si los nombres se repiten (ej. "dropout_1").
# Verifica los nombres exactos con `base_model.summary()`.
try:
    # Intenta obtener las capas con nombres por defecto
    base_model.get_layer("global_average_pooling1d").trainable = True
    base_model.get_layer("dropout").trainable = True
    base_model.get_layer("dense_2").trainable = True # La primera Dense del clasificador
    base_model.get_layer("dropout_1").trainable = True
    base_model.get_layer("output").trainable = True    # La capa de salida SIEMPRE debe ser entrenable
except ValueError as e:
    print(f"\n‚ö†Ô∏è  Error al obtener capa: {e}")
    print("Verifica los nombres de las capas en el resumen del modelo impreso arriba y aj√∫stalos en el script.")
    exit()


print("\nCapas que ser√°n re-entrenadas (fine-tuning):")
for layer in base_model.layers:
    if layer.trainable:
        print(f"  -> {layer.name}")

print("\nPar√°metros entrenables ANTES del fine-tuning:", np.sum([np.prod(v.shape) for v in base_model.trainable_variables]))

# --- 5. Re-compilar el Modelo ---
print(f"\n‚öôÔ∏è  Re-compilando modelo con una tasa de aprendizaje baja (lr={FINETUNE_LR})")
base_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=FINETUNE_LR),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# --- 6. Ejecutar Fine-Tuning ---
print(f"\nüöÄ Iniciando fine-tuning por {FINETUNE_EPOCHS} √©pocas...")
history = base_model.fit(
    train_ds_h,
    epochs=FINETUNE_EPOCHS,
    validation_data=test_ds_h, # Usamos el test set como validaci√≥n en este ejemplo
    verbose=1
)

# --- 7. Evaluaci√≥n Final ---
print("\nüìà Evaluando el modelo despu√©s del fine-tuning...")
loss, accuracy = base_model.evaluate(test_ds_h, verbose=0)
print(f"  -> Precisi√≥n final en Test: {accuracy:.2%}")